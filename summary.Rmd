---
author: "Adamma Morrison"
date: "3/24/2018"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# First download the file
url <- "https://www.collegeparkmd.gov/document_center/Admin/CityManager/Communications/College%20Park%20Community%20Survey%20Report%20FINAL%20web.pdf"
download.file(url, "surveyresults.pdf")
# Use the pdftools package to open pdf
library(pdftools)
library(tidyverse)
txt <- pdf_text("surveyresults.pdf") # entire text of document
toc <- pdf_toc("surveyresults.pdf") # table of contents of the document
# Extract the sections of the survey results
out <- vector("list", length = 8)
for (i in 1:8) {
        out[[i]] <- toc[2]$children[[5]]$children[[i]]$title
}
survey.sections <- unlist(out) #contains the 8 topics that we should focus on

# Use lexRankr package to summarize text for a section
library(lexRankr)
# This function will pull the top sentences for each section.  Enter the number of sentences to pull, and the document.  Can be used on sections of document.
get_summary <- function(document, totalsentenceswanted = 5) {
        top_10 = lexRankr::lexRank(document,
                                   #only 1 article; repeat same docid for all of input vector
                                   docId = rep(1, length(document)),
                                   #return 3 sentences to mimick /u/autotldr's output
                                   n = totalsentenceswanted,
                                   continuous = TRUE)
        
        #reorder the top 3 sentences to be in order of appearance in article
        order_of_appearance = order(as.integer(gsub("_","",top_10$sentenceId)))
        #extract sentences in order of appearance
        ordered_top_10 = top_10[order_of_appearance, "sentence"]
        return(ordered_top_10)
}

# Pull summaries for each section
library(tidyverse)
survey.sections[1] # on pages 5-10
x <- get_summary(txt[5:10])
(gsub("\n", " ", x)) %>% 
        paste(collapse = " ")

survey.sections[2] # on pages 11-15 
y <- get_summary(txt[11:15], 8)
(gsub("\n", " ", y[c(1,4,7)])) %>% 
        paste(collapse = " ")

survey.sections[3] # on pages 16-18
z <- get_summary(txt[16:18], 10)
(gsub("\n", " ", z[c(1,2,3,7,9)])) %>% 
        paste(collapse = " ")

survey.sections[4] # on pages 19-20
a <- get_summary(txt[19:20], 10)
(gsub("\n", " ", a[c(1,4,5,7)])) %>% 
        paste(collapse = " ")

survey.sections[5] # on pages 21-27
b <- get_summary(txt[21:27], 10)
(gsub("\n", " ", b[c(1,3, 5, 9)])) %>% 
        paste(collapse = " ")

survey.sections[6] # on pages 28-29
c <- get_summary(txt[28:29], 10)
(gsub("\n", " ", c[c(1,2, 3, 4, 6)])) %>% 
        paste(collapse = " ")

survey.sections[7] # on page 30
d <- get_summary(txt[30], 10)
(gsub("\n", " ", d[c(1:6)])) %>% 
        paste(collapse = " ")

survey.sections[8] # on page 31
e <- get_summary(txt[31], 1)
(gsub("\n", " ", e)) %>% 
        paste(collapse = " ")



```

## Summary of College Park 2017 Community Survey, using R

After downloading, the pdftools package was used to extract text from the pdf file.  
```{r, eval=FALSE}
# First download the file
url <- "https://www.collegeparkmd.gov/document_center/Admin/CityManager/Communications/College%20Park%20Community%20Survey%20Report%20FINAL%20web.pdf"
download.file(url, "surveyresults.pdf")
# Use the pdftools package to open pdf
library(pdftools)
txt <- pdf_text("surveyresults.pdf") # entire text of document
toc <- pdf_toc("surveyresults.pdf") # table of contents of the document
```
Use a for loop to pull out the eight subsections of the survey results.  
``` {r, eval=F}
# Extract the sections of the survey results
out <- vector("list", length = 8)
for (i in 1:8) {
        out[[i]] <- toc[2]$children[[5]]$children[[i]]$title
}
survey.sections <- unlist(out) #contains the 8 topics that we should focus on
```
The eight topics that were covered under the survey
```{r, echo=F}
survey.sections
```
Now use the lexRankr package to generate summary.  The function below will let the user choose the total number of sentences in the summary, and the exact pages that should be used.  By default, this function will pull top 5 sentences.
```{r, eval=F}
library(lexRankr)
# This function will pull the top sentences for each section.  Enter the number of sentences to pull, and the document.  Can be used on sections of document.
get_summary <- function(document, totalsentenceswanted = 5) {
        top_10 = lexRankr::lexRank(document,
                                   docId = rep(1, length(document)),
                                   n = totalsentenceswanted,
                                   continuous = TRUE)
        
        #reorder the top sentences to be in order of appearance in article
        order_of_appearance = order(as.integer(gsub("_","",top_10$sentenceId)))
        #extract sentences in order of appearance
        ordered_top_10 = top_10[order_of_appearance, "sentence"]
        return(ordered_top_10)
}

```
## Results

1. `r survey.sections[1] # on pages 5-10`
```{r, echo=FALSE, message=F}
(gsub("\n", " ", x)) %>% 
        paste(collapse = " ")
```

2. `r survey.sections[2] # on pages 11-15`
```{r, echo=FALSE, message=F}
(gsub("\n", " ", y[c(1,4,7)])) %>% 
        paste(collapse = " ")
```
        
3. `r survey.sections[3] # on pages 16-18`
```{r, echo=FALSE, message=F}
(gsub("\n", " ", z[c(1,2,3,7,9)])) %>% 
        paste(collapse = " ")
```

4. `r survey.sections[4] # on pages 19-20`
```{r, echo=FALSE, message=F}
(gsub("\n", " ", a[c(1,4,5,7)])) %>% 
        paste(collapse = " ")
```

5. `r survey.sections[5] # on pages 21-27`
```{r, echo=FALSE, message=F}
(gsub("\n", " ", b[c(1,3, 5, 9)])) %>% 
        paste(collapse = " ")
```

6. `r survey.sections[6] # on pages 28-29`
```{r, echo=FALSE, message=F}
(gsub("\n", " ", c[c(1,2, 3, 4, 6)])) %>% 
        paste(collapse = " ")
```

7. `r survey.sections[7] # on page 30`
```{r, echo=FALSE, message=F}
(gsub("\n", " ", d[c(1:6)])) %>% 
        paste(collapse = " ")
```

8. `r survey.sections[8] # on page 31`
```{r, echo=FALSE, message=F}
(gsub("\n", " ", e[1])) %>% 
        paste(collapse = " ")
```

Suggestions for future analysis: The Verbatim Responses to Open-ended Questions on pages 57- 63 contain specific requests from residents.  A heirarchical cluster analysis of these responses would be the best way to summarize and visualize suggestions.
